{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python :3.7.0 (default, Jun 28 2018, 08:04:48) [MSC v.1912 64 bit (AMD64)]\n",
      "nlkt :3.4.4\n",
      "sklearn:0.21.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "\n",
    "\n",
    "print(\"Python :{}\".format(sys.version))\n",
    "print(\"nlkt :{}\".format(nltk.__version__))\n",
    "print(\"sklearn:{}\".format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus -\n",
    "Body of text, singular. Corpora is the plural of this.Example: A collection of medical journals.\n",
    "\n",
    "### Lexicon -\n",
    "Words and their meanings. Example: English Dictionary. Consider that different fields will have different lexicons.\n",
    "\n",
    "### Token -\n",
    "Each entity that is a part of whatever was split up based on rules.For example, each word is a token when a sentence is tokenized into words. Each sentence can also be a token, if you tokenized the sentences out of paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Students, how are yoy doing today?', 'The olympics are inspiring, and python is awsome.', 'You look great today.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"Hello Students, how are yoy doing today? The olympics are inspiring, and python is awsome. You look great today.\"\n",
    "\n",
    "print(sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Students', ',', 'how', 'are', 'yoy', 'doing', 'today', '?', 'The', 'olympics', 'are', 'inspiring', ',', 'and', 'python', 'is', 'awsome', '.', 'You', 'look', 'great', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'needn', \"isn't\", 's', 'through', 'don', 'only', \"don't\", 'doing', 'an', 't', 'as', 'that', 'when', 've', \"wasn't\", \"mustn't\", 'under', 'those', 'y', 'didn', \"didn't\", 'to', 'some', 'them', 'she', 'the', 'shan', 'wouldn', 'll', 'yourself', 'did', 'than', \"needn't\", 'me', 'if', 'for', 'just', 'ourselves', 'was', 'then', 'once', \"you'd\", 'your', 'hasn', 'herself', 'this', 'all', 'any', 'which', 'nor', 'so', 'theirs', 'they', 'but', 'have', 'd', 'am', 'are', 'aren', 'more', 'where', 'should', 'mightn', 'off', 'myself', \"you're\", 'my', 'not', 'between', 'own', 'themselves', 'we', 'hadn', 'whom', 'is', 'why', 'yours', \"she's\", 'from', 'won', \"aren't\", 'been', 'because', 'with', 'ma', 'before', 'yourselves', 'during', 'on', 'up', 'couldn', \"won't\", \"it's\", 'further', 'most', 'does', \"that'll\", 'be', 'their', 'what', 'ain', 'itself', 'his', 'while', 'too', 'a', 'can', \"weren't\", 'such', 'were', 'out', 're', 'mustn', 'until', 'shouldn', 'again', 'being', \"couldn't\", 'each', 'of', 'her', \"shan't\", 'about', 'i', \"you'll\", 'will', 'down', 'ours', 'at', 'now', 'these', 'after', 'doesn', 'or', 'by', 'other', \"doesn't\", 'and', 'over', 'm', \"hadn't\", \"hasn't\", 'very', 'below', \"mightn't\", \"wouldn't\", \"haven't\", 'he', 'do', 'wasn', 'isn', 'its', \"shouldn't\", 'both', 'having', 'has', 'had', 'there', 'haven', 'our', 'him', \"should've\", 'above', 'few', 'it', 'here', 'who', 'hers', 'how', 'himself', 'you', 'in', \"you've\", 'against', 'no', 'into', 'same', 'o', 'weren'}\n"
     ]
    }
   ],
   "source": [
    "#Removing useless words or stop words\n",
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('English')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sample', 'text', ',', 'showing', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "example = 'This is the sample text, showing of the filtration.'\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "words = word_tokenize(example)\n",
    "sentence =[w for w in words if not w in stopwords]\n",
    "\n",
    "\n",
    "#filtered_sentence = []\n",
    "#for w in words:\n",
    "    #if w not in stopwords:\n",
    "         #filtered_sentence.append(w)\n",
    "\n",
    "print(sentence)\n",
    "#print(filtered_sentence)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride\n",
      "ride\n",
      "rider\n",
      "ride\n"
     ]
    }
   ],
   "source": [
    "#Stemming words with nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "example_words = ['ride','riding','rider','rides']\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when\n",
      "rider\n",
      "are\n",
      "ride\n",
      "their\n",
      "hors\n",
      ",\n",
      "they\n",
      "often\n",
      "think\n",
      "of\n",
      "how\n",
      "cowboy\n",
      "rode\n",
      "their\n",
      "hors\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#Stemming sentences with nltk\n",
    "new_text = 'When riders are riding their horses, they often think of how cowboys rode their horses.'\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "print(udhr.raw('English-Latin1'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "\n",
    "train_text  = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "print(train_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have some text, we can train the PunktSentenceTokenizer\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets tokenize the sample text\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#Define a function that will tells us which part of speech each tokenized word is\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged =nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "            \n",
    "    except Exception as e:\n",
    "            print(str(e))\n",
    "            \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the tags\n",
    "#nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "(Chunk ADDRESS/NNP)\n",
      "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "(Chunk THE/NNP UNION/NNP January/NNP)\n",
      "(Chunk THE/NNP PRESIDENT/NNP)\n",
      "(Chunk Thank/NNP)\n",
      "(Chunk Mr./NNP Speaker/NNP)\n",
      "(Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Supreme/NNP Court/NNP)\n",
      "(Chunk called/VBD America/NNP)\n",
      "(Chunk Coretta/NNP Scott/NNP King/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "(Chunk State/NNP)\n",
      "(Chunk Union/NNP Address/NNP)\n",
      "(Chunk Capitol/NNP)\n",
      "(Chunk Tuesday/NNP)\n",
      "(Chunk Jan/NNP)\n",
      "(Chunk White/NNP House/NNP photo/NN)\n",
      "(Chunk Eric/NNP DraperEvery/NNP time/NN)\n",
      "(Chunk Capitol/NNP dome/NN)\n",
      "(Chunk have/VBP served/VBN America/NNP)\n",
      "(Chunk Tonight/NNP)\n",
      "(Chunk Union/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk United/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk September/NNP)\n",
      "(Chunk Dictatorships/NNP shelter/NN)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Afghanistan/NNP)\n",
      "(Chunk Iraqis/NNP)\n",
      "(Chunk Lebanon/NNP)\n",
      "(Chunk Egypt/NNP)\n",
      "(Chunk Syria/NNP)\n",
      "(Chunk Burma/NNP)\n",
      "(Chunk Zimbabwe/NNP)\n",
      "(Chunk North/NNP Korea/NNP)\n",
      "(Chunk Iran/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "(Chunk Union/NNP Address/NNP)\n",
      "(Chunk Capitol/NNP)\n",
      "(Chunk Tuesday/NNP)\n",
      "(Chunk Jan/NNP)\n",
      "(Chunk White/NNP House/NNP photo/NN)\n",
      "(Chunk Eric/NNP Draper/NNP No/NNP one/NN)\n",
      "(Chunk Islam/NNP)\n",
      "(Chunk Laden/NNP)\n",
      "(Chunk Middle/NNP East/NNP)\n",
      "(Chunk Iraq/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Beslan/NNP)\n",
      "(Chunk London/NNP)\n",
      "(Chunk Earth/NNP)\n",
      "(Chunk Applause/NNP)\n"
     ]
    }
   ],
   "source": [
    "#Doing the chunking. Basically chunking is combining the same part of speech tags together if they come together in a sentence in the form of a chunk\n",
    "\n",
    "train_text  = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "\n",
    "\n",
    "#Now that we have some text, we can train the PunktSentenceTokenizer\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "\n",
    "#Now lets tokenize the sample text\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "\n",
    "\n",
    "#Define a function that will tells us which part of speech each tokenized word is\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:50]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged =nltk.pos_tag(words)\n",
    "            #Combine the part of speech tag with a regular expression\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            #print the nltk tree\n",
    "            for subtree in chunked.subtrees(filter = lambda t:t.label()=='Chunk'):\n",
    "                print(subtree)\n",
    "            #draw the chunks with nltk\n",
    "            #chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "            print(str(e))\n",
    "            \n",
    "process_content()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "<RB.?>* = \"0 or more of any tense of adverb\"followed by:\n",
    "<VB.?>* = \"0 or more of any tense of verb\" followed by:\n",
    "<NNP>+ = \"One or more proper nouns\" followed by:\n",
    "<NN>? = \"Zero or one singular noun\"\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "+ = match 1 or more\n",
    "? = match 0 or 1 repetiitions\n",
    "* = match 0 or more repetitions\n",
    ". = Any character except a new line.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP 'S/POS ADDRESS/NNP)\n",
      "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "(Chunk\n",
      "  THE/NNP\n",
      "  UNION/NNP\n",
      "  January/NNP\n",
      "  31/CD\n",
      "  ,/,\n",
      "  2006/CD\n",
      "  THE/NNP\n",
      "  PRESIDENT/NNP\n",
      "  :/:\n",
      "  Thank/NNP\n",
      "  you/PRP)\n",
      "(Chunk ./.)\n",
      "(Chunk\n",
      "  Mr./NNP\n",
      "  Speaker/NNP\n",
      "  ,/,\n",
      "  Vice/NNP\n",
      "  President/NNP\n",
      "  Cheney/NNP\n",
      "  ,/,\n",
      "  members/NNS)\n",
      "(Chunk Congress/NNP ,/, members/NNS)\n",
      "(Chunk\n",
      "  Supreme/NNP\n",
      "  Court/NNP\n",
      "  and/CC\n",
      "  diplomatic/JJ\n",
      "  corps/NN\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:)\n",
      "(Chunk our/PRP$ nation/NN lost/VBD)\n",
      "(Chunk\n",
      "  beloved/VBN\n",
      "  ,/,\n",
      "  graceful/JJ\n",
      "  ,/,\n",
      "  courageous/JJ\n",
      "  woman/NN\n",
      "  who/WP\n",
      "  called/VBD\n",
      "  America/NNP)\n",
      "(Chunk its/PRP$ founding/NN ideals/NNS and/CC carried/VBD)\n",
      "(Chunk noble/JJ dream/NN ./.)\n",
      "(Chunk Tonight/NN we/PRP are/VBP comforted/VBN)\n",
      "(Chunk hope/NN)\n",
      "(Chunk glad/JJ reunion/NN)\n",
      "(Chunk\n",
      "  husband/NN\n",
      "  who/WP\n",
      "  was/VBD\n",
      "  taken/VBN\n",
      "  so/RB\n",
      "  long/RB\n",
      "  ago/RB\n",
      "  ,/,\n",
      "  and/CC\n",
      "  we/PRP\n",
      "  are/VBP\n",
      "  grateful/JJ)\n",
      "(Chunk good/JJ life/NN)\n",
      "(Chunk Coretta/NNP Scott/NNP King/NNP ./.)\n",
      "(Chunk (/( Applause/NNP ./. )/))\n",
      "(Chunk President/NNP George/NNP W./NNP Bush/NNP reacts/VBZ)\n",
      "(Chunk his/PRP$ State/NNP)\n",
      "(Chunk Union/NNP Address/NNP)\n",
      "(Chunk Capitol/NNP ,/, Tuesday/NNP ,/, Jan/NNP ./.)\n",
      "(Chunk 31/CD ,/, 2006/CD ./.)\n",
      "(Chunk White/NNP House/NNP photo/NN)\n",
      "(Chunk Eric/NNP DraperEvery/NNP time/NN I/PRP 'm/VBP invited/JJ)\n",
      "(Chunk rostrum/NN ,/, I/PRP 'm/VBP humbled/VBN)\n",
      "(Chunk privilege/NN ,/, and/CC mindful/NN)\n",
      "(Chunk history/NN we/PRP 've/VBP seen/VBN together/RB ./.)\n",
      "(Chunk We/PRP have/VBP gathered/VBN)\n",
      "(Chunk Capitol/NNP dome/NN)\n",
      "(Chunk moments/NNS)\n",
      "(Chunk national/JJ mourning/NN and/CC national/JJ achievement/NN ./.)\n",
      "(Chunk We/PRP have/VBP served/VBN America/NNP)\n",
      "(Chunk one/CD)\n",
      "(Chunk most/RBS consequential/JJ periods/NNS)\n",
      "(Chunk\n",
      "  our/PRP$\n",
      "  history/NN\n",
      "  --/:\n",
      "  and/CC\n",
      "  it/PRP\n",
      "  has/VBZ\n",
      "  been/VBN\n",
      "  my/PRP$\n",
      "  honor/NN)\n",
      "(Chunk you/PRP ./.)\n",
      "(Chunk system/NN)\n",
      "(Chunk\n",
      "  two/CD\n",
      "  parties/NNS\n",
      "  ,/,\n",
      "  two/CD\n",
      "  chambers/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  two/CD\n",
      "  elected/JJ\n",
      "  branches/NNS\n",
      "  ,/,\n",
      "  there/EX\n",
      "  will/MD\n",
      "  always/RB)\n",
      "(Chunk differences/NNS and/CC debate/NN ./.)\n",
      "(Chunk But/CC even/RB tough/JJ debates/NNS can/MD)\n",
      "(Chunk conducted/VBN)\n",
      "(Chunk\n",
      "  civil/JJ\n",
      "  tone/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  our/PRP$\n",
      "  differences/NNS\n",
      "  can/MD\n",
      "  not/RB)\n",
      "(Chunk allowed/VBN)\n",
      "(Chunk anger/NN ./.)\n",
      "(Chunk great/JJ issues/NNS)\n",
      "(Chunk us/PRP ,/, we/PRP must/MD)\n",
      "(Chunk spirit/NN)\n",
      "(Chunk goodwill/NN and/CC respect/NN)\n",
      "(Chunk one/CD)\n",
      "(Chunk --/: and/CC I/PRP will/MD)\n",
      "(Chunk my/PRP$ part/NN ./.)\n",
      "(Chunk Tonight/NNP)\n",
      "(Chunk state/NN)\n",
      "(Chunk\n",
      "  our/PRP$\n",
      "  Union/NNP\n",
      "  is/VBZ\n",
      "  strong/JJ\n",
      "  --/:\n",
      "  and/CC\n",
      "  together/RB\n",
      "  we/PRP\n",
      "  will/MD)\n",
      "(Chunk it/PRP stronger/JJR ./.)\n",
      "(Chunk (/( Applause/NNP ./. )/))\n",
      "(Chunk decisive/JJ year/NN ,/, you/PRP and/CC I/PRP will/MD)\n",
      "(Chunk choices/NNS that/WDT determine/VBP)\n",
      "(Chunk future/NN and/CC)\n",
      "(Chunk character/NN)\n",
      "(Chunk our/PRP$ country/NN ./.)\n",
      "(Chunk We/PRP will/MD)\n",
      "(Chunk confidently/RB)\n",
      "(Chunk pursuing/VBG)\n",
      "(Chunk enemies/NNS)\n",
      "(Chunk freedom/NN --/: or/CC retreat/NN)\n",
      "(Chunk our/PRP$ duties/NNS)\n",
      "(Chunk hope/NN)\n",
      "(Chunk easier/JJR life/NN ./.)\n",
      "(Chunk We/PRP will/MD)\n",
      "(Chunk our/PRP$ prosperity/NN)\n",
      "(Chunk leading/VBG)\n",
      "(Chunk world/NN economy/NN --/: or/CC)\n",
      "(Chunk ourselves/PRP off/RP)\n",
      "(Chunk trade/NN and/CC opportunity/NN ./.)\n",
      "(Chunk complex/JJ and/CC challenging/JJ time/NN ,/,)\n",
      "(Chunk road/NN)\n",
      "(Chunk isolationism/NN and/CC protectionism/NN may/MD)\n",
      "(Chunk broad/JJ and/CC inviting/NN --/: yet/CC it/PRP ends/VBZ)\n",
      "(Chunk danger/NN and/CC decline/NN ./.)\n",
      "(Chunk only/JJ way/NN)\n",
      "(Chunk our/PRP$ people/NNS ,/,)\n",
      "(Chunk only/JJ way/NN)\n",
      "(Chunk peace/NN ,/,)\n",
      "(Chunk only/JJ way/NN)\n",
      "(Chunk our/PRP$ destiny/NN is/VBZ)\n",
      "(Chunk our/PRP$ leadership/NN --/:)\n",
      "(Chunk United/NNP States/NNPS)\n",
      "(Chunk America/NNP will/MD)\n",
      "(Chunk ./.)\n",
      "(Chunk (/( Applause/NNP ./. )/))\n"
     ]
    }
   ],
   "source": [
    "#Chinking with nltk\n",
    "\n",
    "train_text  = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "\n",
    "\n",
    "#Now that we have some text, we can train the PunktSentenceTokenizer\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "\n",
    "#Now lets tokenize the sample text\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "\n",
    "\n",
    "#Define a function that will tells us which part of speech each tokenized word is\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:20]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged =nltk.pos_tag(words)\n",
    "            #Combine the part of speech tag with a regular expression\n",
    "            #Defining the chinking pattern\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                          }<V.?|IN|DT|TO>+{\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            #print the nltk tree\n",
    "            #print the chinked\n",
    "            for subtree in chunked.subtrees(filter = lambda t:t.label()=='Chunk'):\n",
    "                print(subtree)\n",
    "            #draw the chunks with nltk\n",
    "            #chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "            print(str(e))\n",
    "            \n",
    "process_content()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:20]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged =nltk.pos_tag(words)\n",
    "            namedent=nltk.ne_chunk(tagged,binary = False)\n",
    "            \n",
    "            #draw the chunks with nltk\n",
    "            namedent.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "            print(str(e))\n",
    "            \n",
    "process_content()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification\n",
    "Now that we have covered the basics of natural language processing, we can move on to text classification using simple machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build list of documents i.e list of movie reviews\n",
    "documents = [(list(movie_reviews.words(fileid)),category)\n",
    "            for category in movie_reviews.categories()\n",
    "            for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "#shuffle the documents\n",
    "random.shuffle(documents)\n",
    "\n",
    "print('Number of Documents:{}'.format(len(documents)))\n",
    "print('First Review:{}'.format(documents[0]))\n",
    "\n",
    "\n",
    "all_words=[]\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "#FreqDist function sorts the words from the most common to the least common    \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "print('Most Common Words:{}'.format(all_words.most_common(15)))\n",
    "print('The word happy :{}'.format(all_words[\"happy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39768\n"
     ]
    }
   ],
   "source": [
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll use the most common 4000 words as features\n",
    "word_features = list(all_words.keys())[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', 'one', 'of', 'the', 'guys', 'dies', 'but', 'his', 'girlfriend', 'continues', 'see', 'him', 'in', 'her', 'life', 'has', 'nightmares', 'what', \"'\", 's', 'deal', '?', 'watch', 'movie', '\"', 'sorta', 'find', 'out', 'critique', 'mind', '-', 'fuck', 'for', 'generation', 'that', 'touches', 'on', 'very', 'cool', 'idea', 'presents', 'it', 'bad', 'package', 'which', 'is', 'makes', 'this', 'review', 'even', 'harder', 'write', 'since', 'i', 'generally', 'applaud', 'films', 'attempt', 'break', 'mold', 'mess', 'with', 'your', 'head', 'such', '(', 'lost', 'highway', '&', 'memento', ')', 'there', 'are', 'good', 'ways', 'making', 'all', 'types', 'these', 'folks', 'just', 'didn', 't', 'snag', 'correctly', 'seem', 'have', 'taken', 'pretty', 'neat', 'concept', 'executed', 'terribly', 'so', 'problems', 'well', 'its', 'main', 'problem', 'simply', 'too', 'jumbled', 'starts', 'off', 'normal', 'downshifts', 'fantasy', 'world', 'you', 'as', 'audience', 'member', 'no', 'going', 'dreams', 'characters', 'coming', 'back', 'from', 'dead', 'others', 'who', 'look', 'like', 'strange', 'apparitions', 'disappearances', 'looooot', 'chase', 'scenes', 'tons', 'weird', 'things', 'happen', 'most', 'not', 'explained', 'now', 'personally', 'don', 'trying', 'unravel', 'film', 'every', 'when', 'does', 'give', 'me', 'same', 'clue', 'over', 'again', 'kind', 'fed', 'up', 'after', 'while', 'biggest', 'obviously', 'got', 'big', 'secret', 'hide', 'seems', 'want', 'completely', 'until', 'final', 'five', 'minutes', 'do', 'make', 'entertaining', 'thrilling', 'or', 'engaging', 'meantime', 'really', 'sad', 'part', 'arrow', 'both', 'dig', 'flicks', 'we', 'actually', 'figured', 'by', 'half', 'way', 'point', 'strangeness', 'did', 'start', 'little', 'bit', 'sense', 'still', 'more', 'guess', 'bottom', 'line', 'movies', 'should', 'always', 'sure', 'before', 'given', 'password', 'enter', 'understanding', 'mean', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'visions', 'about', '20', 'throughout', 'plain', 'lazy', '!', 'okay', 'people', 'chasing', 'know', 'need', 'how', 'giving', 'us', 'different', 'offering', 'further', 'insight', 'down', 'apparently', 'studio', 'took', 'director', 'chopped', 'themselves', 'shows', 'might', 've', 'been', 'decent', 'here', 'somewhere', 'suits', 'decided', 'turning', 'music', 'video', 'edge', 'would', 'actors', 'although', 'wes', 'bentley', 'seemed', 'be', 'playing', 'exact', 'character', 'he', 'american', 'beauty', 'only', 'new', 'neighborhood', 'my', 'kudos', 'holds', 'own', 'entire', 'feeling', 'unraveling', 'overall', 'doesn', 'stick', 'because', 'entertain', 'confusing', 'rarely', 'excites', 'feels', 'redundant', 'runtime', 'despite', 'ending', 'explanation', 'craziness', 'came', 'oh', 'horror', 'slasher', 'flick', 'packaged', 'someone', 'assuming', 'genre', 'hot', 'kids', 'also', 'wrapped', 'production', 'years', 'ago', 'sitting', 'shelves', 'ever', 'whatever', 'skip', 'where', 'joblo', 'nightmare', 'elm', 'street', '3', '7', '/', '10', 'blair', 'witch', '2', 'crow', '9', 'salvation', '4', 'stir', 'echoes', '8']\n"
     ]
    }
   ],
   "source": [
    "#Build a find_features function that will determine which of the word features are contained in a review\n",
    "\n",
    "# The find_features function will determine which of the 3000 word features are contained in the review\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "# Lets use an example from a negative review\n",
    "features = find_features(movie_reviews.words('neg/cv000_29416.txt'))\n",
    "for key, value in features.items():\n",
    "       if value == True:\n",
    "        print (key)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets do it for all the documents. Featureset for every document contains most common 4000 words and true or false infront of them\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can split the featuresets into training and testing datasets using sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "# define a seed for reproducibility\n",
    "seed = 1\n",
    "\n",
    "# split the data into training and testing datasets\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(training))\n",
    "print(len(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy: 82.8\n"
     ]
    }
   ],
   "source": [
    "# We can use sklearn algorithms in NLTK\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SklearnClassifier(SVC(kernel = 'linear'))\n",
    "\n",
    "# train the model on the training data\n",
    "model.train(training)\n",
    "\n",
    "# and test on the testing dataset!\n",
    "accuracy = nltk.classify.accuracy(model, testing)*100\n",
    "print(\"SVC Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
